{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "* PEFT technique: LoRA\n",
    "* Model: distilbert-base-german-cased\n",
    "* Evaluation approach: Accuracy\n",
    "* Fine-tuning dataset: SH108/german-court-decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f551c63a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/student/.local/lib/python3.10/site-packages (2.15.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/student/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/student/.local/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.15.0\n",
      "    Uninstalling datasets-2.15.0:\n",
      "      Successfully uninstalled datasets-2.15.0\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed datasets-3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Fix - See https://knowledge.udacity.com/questions/1060123\n",
    "\n",
    "!pip install -U datasets\n",
    "\n",
    "!pip install -q \"datasets==2.15.0\"\n",
    "\n",
    "# Reload kernel after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85cf7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_name = \"sms_spam\"\n",
    "#text_field = \"sms\"\n",
    "#limit_data = \"\" #unlimited\n",
    "#label_key = \"label\"\n",
    "#remove_columns = []\n",
    "#id2label=dict({0: \"not spam\", 1: \"spam\"}),\n",
    "#label2id=dict({\"not spam\": 0, \"spam\": 1}),\n",
    "#model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# =========================================\n",
    "\n",
    "dataset_name=\"SH108/german-court-decisions\"\n",
    "text_field=\"decision\"\n",
    "limit_data=\"[:10000]\"\n",
    "label_key = \"convicted\"\n",
    "remove_columns = ['court', 'state', 'costs to defendant', 'dismissed', 'date', 'costs to plaintiff', 'offense']\n",
    "id2label={0: \"not convicted\", 1: \"convicted\"}\n",
    "label2id={\"not convicted\": 0, \"convicted\": 1}\n",
    "model_name=\"distilbert-base-german-cased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['decision', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['decision', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Since the dataset contains only a \"train\" split create the test / train split\n",
    "ds = load_dataset(dataset_name, split=f\"train{limit_data}\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "\n",
    "# Remove some columns\n",
    "ds = ds.remove_columns(remove_columns)\n",
    "\n",
    "# Enable label\n",
    "if label_key is not None and label_key != \"label\":\n",
    "    ds = ds.rename_column(label_key, \"label\")\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6584418",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1348015345ff4730a04c6e473a18fce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "ds_tokenized = {}\n",
    "for split in splits:\n",
    "    ds_tokenized[split] = ds[split].map(\n",
    "        lambda x: tokenizer(x[text_field], truncation=True, padding=\"max_length\"), batched=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['decision', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 8000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['decision', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 2000\n",
       " })}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730fdd17",
   "metadata": {},
   "source": [
    "### Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d294e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    force_download=False, #Suppress FutureWarning\n",
    ")\n",
    "\n",
    "# Freeze parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d813a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df7f8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory for Training Arguments\n",
    "from transformers import TrainingArguments\n",
    "def create_training_args(output_dir):\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        #remove_unused_columns=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dff186c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training without PEFT / LoRA\n",
    "import numpy as np\n",
    "\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=create_training_args(f\"./data/{model_name}_{dataset_name}\"),\n",
    "    train_dataset=ds_tokenized[\"train\"],\n",
    "    eval_dataset=ds_tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cafe110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 03:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.750350832939148,\n",
       " 'eval_accuracy': 0.1035,\n",
       " 'eval_runtime': 34.1987,\n",
       " 'eval_samples_per_second': 58.482,\n",
       " 'eval_steps_per_second': 3.655}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get untrained performance\n",
    "performance_pretrained = trainer.evaluate()\n",
    "performance_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4cb0068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 05:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>0.167689</td>\n",
       "      <td>0.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.153241</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/distilbert-base-german-cased_SH108/german-court-decisions/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/distilbert-base-german-cased_SH108/german-court-decisions/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.20341574096679688, metrics={'train_runtime': 352.8866, 'train_samples_per_second': 45.34, 'train_steps_per_second': 2.834, 'total_flos': 2119478378496000.0, 'train_loss': 0.20341574096679688, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eefcf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1532410830259323,\n",
       " 'eval_accuracy': 0.932,\n",
       " 'eval_runtime': 33.7886,\n",
       " 'eval_samples_per_second': 59.192,\n",
       " 'eval_steps_per_second': 3.699,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_trained = trainer.evaluate()\n",
    "performance_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47657ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,257,988 || all params: 68,066,308 || trainable%: 1.8481801598523604\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_lora_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    force_download=False, #Suppress FutureWarning\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "# Create a PEFT Config for LoRA\n",
    "# See https://knowledge.udacity.com/questions/1027860\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=['v_lin'],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# Get trainable PEFT model\n",
    "from peft import get_peft_model\n",
    "model_lora = get_peft_model(model_lora_base, lora_config)\n",
    "\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48e3072c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 12:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.079775</td>\n",
       "      <td>0.968500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.058249</td>\n",
       "      <td>0.973500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/distilbert-base-german-cased_SH108/german-court-decisions_lora/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/distilbert-base-german-cased_SH108/german-court-decisions_lora/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.1252991943359375, metrics={'train_runtime': 770.9133, 'train_samples_per_second': 20.755, 'train_steps_per_second': 1.297, 'total_flos': 2152206630912000.0, 'train_loss': 0.1252991943359375, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=create_training_args(f\"./data/{model_name}_{dataset_name}_lora\"),\n",
    "    train_dataset=ds_tokenized[\"train\"],\n",
    "    eval_dataset=ds_tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f12dde57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05824864283204079,\n",
       " 'eval_accuracy': 0.9735,\n",
       " 'eval_runtime': 41.7284,\n",
       " 'eval_samples_per_second': 47.929,\n",
       " 'eval_steps_per_second': 2.996,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_lora = trainer_lora.evaluate()\n",
    "performance_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.save_pretrained(model_name + \"-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(model_name + \"-lora\", force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: convicted\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,force_download=False)\n",
    "\n",
    "decision=\"\"\"\n",
    "1.\n",
    "Die Beklagte wird verurteilt, an die Klägerin 1.822,96 € für das vorgerichtliche Abmahnschreiben zuzüglich Zinsen in Höhe von 5 Prozentpunkten über dem jeweiligen Basiszinssatz seit dem 13.10.2017 zu zahlen.\n",
    "2.\n",
    "Wegen der Zahlungsmehrforderung wird die Klage abgewiesen.\n",
    "3.\n",
    "Die Beklagte hat die Kosten des Rechtsstreits zu tragen.\n",
    "4.\n",
    "Das Urteil ist für die Klägerin vorläufig vollstreckbar gegen Sicherheitsleistung in Höhe von 110 % des jeweils zu vollstreckenden Betrages.\n",
    "Beschluss\n",
    "Der Streitwert wird auf 60.000,00 € festgesetzt.\n",
    "\"\"\"\n",
    "inputs = tokenizer(decision, return_tensors=\"pt\")\n",
    "    \n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model(**inputs).logits\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities)\n",
    "    \n",
    "# Print result\n",
    "print(f\"Classification: {id2label[int(predicted_class)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b31ea8",
   "metadata": {},
   "source": [
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pretrained</th>\n",
       "      <th>normal</th>\n",
       "      <th>LoRA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.750351</td>\n",
       "      <td>0.153241</td>\n",
       "      <td>0.058249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_runtime</th>\n",
       "      <td>34.198700</td>\n",
       "      <td>33.788600</td>\n",
       "      <td>41.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <td>58.482000</td>\n",
       "      <td>59.192000</td>\n",
       "      <td>47.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <td>3.655000</td>\n",
       "      <td>3.699000</td>\n",
       "      <td>2.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         pretrained     normal       LoRA\n",
       "eval_loss                  0.750351   0.153241   0.058249\n",
       "eval_accuracy              0.103500   0.932000   0.973500\n",
       "eval_runtime              34.198700  33.788600  41.728400\n",
       "eval_samples_per_second   58.482000  59.192000  47.929000\n",
       "eval_steps_per_second      3.655000   3.699000   2.996000\n",
       "epoch                           NaN   2.000000   2.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "comparision = pd.DataFrame.from_dict(\n",
    "    {\"pretrained\": performance_pretrained,\n",
    "     \"normal\": performance_trained, \n",
    "     \"LoRA\": performance_lora}\n",
    ")\n",
    "comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a398775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-german-cased-lora/\n",
      "distilbert-base-german-cased-lora/adapter_model.bin\n",
      "distilbert-base-german-cased-lora/adapter_config.json\n",
      "distilbert-base-german-cased-lora/README.md\n"
     ]
    }
   ],
   "source": [
    "!tar chvfz distilbert-base-german-cased-lora.tar.gz distilbert-base-german-cased-lora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
